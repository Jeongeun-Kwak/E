{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tight-buffer",
   "metadata": {},
   "source": [
    "# E6 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-mongolia",
   "metadata": {},
   "source": [
    "일시: 2020.01.26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-gauge",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-reunion",
   "metadata": {},
   "source": [
    "- 파일을 다운로드 받아 압축을 풀어줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-waterproof",
   "metadata": {},
   "source": [
    "```\n",
    "$ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "$ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-queen",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-titanium",
   "metadata": {},
   "source": [
    "- glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "crude-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['I. LIFE.', '', '']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러 개의 txt 파일을 모두 읽어서 raw_corpus에 담기.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-publicity",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ahead-advantage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I. LIFE.\n",
      "I.\n",
      "SUCCESS.\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜀.\n",
    "\n",
    "    if idx > 9: break   # 10개 문장 확인.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "democratic-brooklyn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 변경, 양쪽 공백 삭제.\n",
    "  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백 추가.\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 공백 패턴을 만나면 스페이스 1개로 치환.\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환.\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장으로 필터링 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "apparent-prophet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> i . life . <end>',\n",
       " '<start> i . <end>',\n",
       " '<start> success . <end>',\n",
       " '<start> published in a masque of poets <end>',\n",
       " '<start> at the request of h . h . , the author s <end>',\n",
       " '<start> fellow townswoman and friend . <end>',\n",
       " '<start> success is counted sweetest <end>',\n",
       " '<start> by those who ne er succeed . <end>',\n",
       " '<start> to comprehend a nectar <end>',\n",
       " '<start> requires sorest need . <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-wells",
   "metadata": {},
   "source": [
    "- ```tokenize()```함수로 데이터를 Tensor로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sticky-ceremony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    5   20 ...    0    0    0]\n",
      " [   2    5   20 ...    0    0    0]\n",
      " [   2 2762   20 ...    0    0    0]\n",
      " ...\n",
      " [   2  240    1 ...    0    0    0]\n",
      " [   2   10  502 ...    0    0    0]\n",
      " [   2  129   21 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f8f48767ad0>\n",
      "(175986, 16)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지 생성.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000, # 전체 단어의 개수. \n",
    "        filters = ' ', # 별도 전처리 로직 없음.\n",
    "        oov_token = \"<unk>\"  # out-of-vocabulary.\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # 구축한 corpus로부터 Tokenizer가 사전을 자동구축.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding 메소드를 제공.\n",
    "    # maxlen는 start와 end가 빠질 것을 감안하여 16개로 지정.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post', maxlen = 16, truncating = 'post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parental-congress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    5   20  102   20    3    0    0    0    0]\n",
      " [   2    5   20    3    0    0    0    0    0    0]\n",
      " [   2 2762   20    3    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10]) # 확인용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "geological-trauma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word: # 인덱스 확인.\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "possible-weapon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   5  20 102  20   3   0   0   0   0   0   0   0   0   0]\n",
      "[  5  20 102  20   3   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내 소스 문장을 생성.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-uruguay",
   "metadata": {},
   "source": [
    "# Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-settlement",
   "metadata": {},
   "source": [
    "- ```sklearn```모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foster-namibia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140788, 15)\n",
      "Target Train: (140788, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-respondent",
   "metadata": {},
   "source": [
    "- 텐서로 ```tf.data.Dataset```객체를 생성합니다. ```tf.data.Dataset.from_tensor_slices()```메소드를 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "human-observation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((256, 15), (256, 15)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((256, 15), (256, 15)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-arctic",
   "metadata": {},
   "source": [
    "# Step 5. 인공지능 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "social-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256 # 조절가능\n",
    "hidden_size = 1024 # 조절가능\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-processor",
   "metadata": {},
   "source": [
    "- 모델의 최종 출력은 shape = (256, 15, 12001)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dirty-mirror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 15, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.77243201e-05,  1.54104418e-05,  2.71609257e-04, ...,\n",
       "          1.90677474e-05, -1.71439839e-04, -1.46155100e-04],\n",
       "        [ 7.98682449e-05,  2.02207593e-05,  2.49341305e-04, ...,\n",
       "         -1.18406915e-05, -5.32128441e-04, -4.27197374e-05],\n",
       "        [ 1.46607214e-04,  9.26985813e-05,  2.04527372e-04, ...,\n",
       "          7.44328281e-06, -7.92041363e-04, -5.30144898e-05],\n",
       "        ...,\n",
       "        [-1.41038297e-04, -4.52314387e-04,  9.20595630e-06, ...,\n",
       "         -4.12322406e-04, -4.50879365e-04, -5.03543066e-04],\n",
       "        [-1.26562110e-04, -5.20382659e-04,  3.19952436e-04, ...,\n",
       "         -4.75726731e-04, -2.63735419e-04, -7.65580568e-04],\n",
       "        [ 5.67763345e-06, -4.78416361e-04,  2.09088757e-04, ...,\n",
       "         -1.66343249e-04,  4.54165856e-05, -7.41570489e-04]],\n",
       "\n",
       "       [[ 1.77243201e-05,  1.54104418e-05,  2.71609257e-04, ...,\n",
       "          1.90677474e-05, -1.71439839e-04, -1.46155100e-04],\n",
       "        [ 1.98155278e-04,  1.62825105e-04,  2.32261067e-04, ...,\n",
       "          7.39189563e-05, -1.37757030e-04, -1.87488738e-04],\n",
       "        [ 2.87796516e-04,  1.24929065e-04,  3.37588019e-04, ...,\n",
       "          1.86487450e-04, -1.27407533e-04, -8.63021705e-05],\n",
       "        ...,\n",
       "        [ 1.71135308e-03, -2.67091673e-04, -3.08022974e-03, ...,\n",
       "          3.72398971e-03,  1.80828676e-03,  5.12837782e-04],\n",
       "        [ 1.74548617e-03, -4.71624429e-04, -3.46347736e-03, ...,\n",
       "          3.98230972e-03,  1.97539967e-03,  6.42294588e-04],\n",
       "        [ 1.75401266e-03, -6.69586647e-04, -3.79959866e-03, ...,\n",
       "          4.19672346e-03,  2.14774045e-03,  7.60099385e-04]],\n",
       "\n",
       "       [[ 1.77243201e-05,  1.54104418e-05,  2.71609257e-04, ...,\n",
       "          1.90677474e-05, -1.71439839e-04, -1.46155100e-04],\n",
       "        [ 9.88482498e-06,  2.41580812e-04,  7.36850896e-04, ...,\n",
       "          1.58766052e-04, -8.43862945e-05, -2.89146788e-04],\n",
       "        [ 2.95989856e-04,  5.50889992e-04,  8.16237065e-04, ...,\n",
       "          3.74427997e-04, -4.37145383e-04, -8.01897491e-04],\n",
       "        ...,\n",
       "        [-5.13093837e-04,  1.40651967e-03,  7.88388366e-04, ...,\n",
       "          1.54399691e-04, -1.07517641e-03, -1.78981060e-03],\n",
       "        [-3.11869313e-04,  1.35229924e-03,  4.76890826e-04, ...,\n",
       "          5.56502549e-04, -7.20844488e-04, -1.40625145e-03],\n",
       "        [-1.85119061e-05,  1.21890730e-03,  3.49601178e-05, ...,\n",
       "          1.01592136e-03, -3.69922607e-04, -1.00293325e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.77243201e-05,  1.54104418e-05,  2.71609257e-04, ...,\n",
       "          1.90677474e-05, -1.71439839e-04, -1.46155100e-04],\n",
       "        [ 1.98155278e-04,  1.62825105e-04,  2.32261067e-04, ...,\n",
       "          7.39189563e-05, -1.37757030e-04, -1.87488738e-04],\n",
       "        [ 6.98836986e-04,  3.48306727e-04, -5.20989415e-05, ...,\n",
       "          3.20512685e-04,  1.70699022e-05, -2.26381817e-05],\n",
       "        ...,\n",
       "        [ 7.68886297e-04,  4.42149962e-04,  4.45293932e-04, ...,\n",
       "          4.50463442e-04,  6.50646631e-04,  2.38253910e-04],\n",
       "        [ 6.24326174e-04,  1.97574846e-05,  5.38758235e-04, ...,\n",
       "          3.45367182e-04,  8.29568831e-04,  2.90510652e-04],\n",
       "        [ 4.46767750e-04,  3.88420594e-05,  5.54433267e-04, ...,\n",
       "          5.92563592e-04,  7.89757294e-04,  3.40813363e-04]],\n",
       "\n",
       "       [[ 1.77243201e-05,  1.54104418e-05,  2.71609257e-04, ...,\n",
       "          1.90677474e-05, -1.71439839e-04, -1.46155100e-04],\n",
       "        [-1.89454731e-04, -2.69662123e-05,  3.38522776e-04, ...,\n",
       "         -6.36666737e-05, -1.72862492e-04, -2.25771393e-04],\n",
       "        [ 1.05420717e-04, -3.23096814e-04,  1.53539862e-04, ...,\n",
       "          4.80320959e-05,  5.29790850e-05, -4.14434995e-04],\n",
       "        ...,\n",
       "        [-3.75657168e-04,  3.66641791e-04, -1.61647869e-04, ...,\n",
       "         -7.42241391e-05,  3.25046130e-04,  1.15032577e-04],\n",
       "        [-3.21430329e-04,  3.73139919e-04, -2.46342912e-04, ...,\n",
       "          1.90344872e-05,  2.26690157e-04, -1.58550683e-04],\n",
       "        [-1.22006255e-04,  6.12175034e-04, -3.14269855e-04, ...,\n",
       "         -1.85739395e-04,  1.59787582e-04, -3.97782453e-04]],\n",
       "\n",
       "       [[ 1.77243201e-05,  1.54104418e-05,  2.71609257e-04, ...,\n",
       "          1.90677474e-05, -1.71439839e-04, -1.46155100e-04],\n",
       "        [ 5.64547081e-05,  1.14072318e-05,  3.52155184e-04, ...,\n",
       "         -1.28775471e-04, -4.38292627e-04,  8.25530879e-06],\n",
       "        [ 6.49136491e-04, -1.92327905e-04,  4.67692502e-04, ...,\n",
       "         -2.47460848e-04, -2.46499578e-04, -3.28988972e-06],\n",
       "        ...,\n",
       "        [ 6.22550840e-04, -9.20045422e-05, -9.52533795e-04, ...,\n",
       "          2.19594408e-03,  8.37689149e-04,  3.49778042e-04],\n",
       "        [ 7.71982712e-04, -1.52312568e-04, -1.47418084e-03, ...,\n",
       "          2.67471489e-03,  9.94669623e-04,  5.56010345e-04],\n",
       "        [ 9.10298550e-04, -2.61286565e-04, -1.96736725e-03, ...,\n",
       "          3.09626339e-03,  1.15586515e-03,  7.29078311e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "offensive-colony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stylish-medline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "549/549 [==============================] - 1286s 2s/step - loss: 3.5156 - val_loss: 3.1688\n",
      "Epoch 2/5\n",
      "549/549 [==============================] - 1243s 2s/step - loss: 3.0610 - val_loss: 2.9782\n",
      "Epoch 3/5\n",
      "549/549 [==============================] - 1200s 2s/step - loss: 2.8927 - val_loss: 2.8627\n",
      "Epoch 4/5\n",
      "549/549 [==============================] - 1181s 2s/step - loss: 2.7682 - val_loss: 2.7847\n",
      "Epoch 5/5\n",
      " 59/549 [==>...........................] - ETA: 16:47 - loss: 2.6784"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8da3a71ac291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     from_logits=True, reduction='none'), optimizer=optimizer)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 10번 시도에 val_loss 2.2 이하 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none'), optimizer=optimizer)\n",
    "\n",
    "history = model.fit(train_dataset, epochs = 5, validation_data = val_dataset, verbose = 1) # 10번 시도에 val_loss 2.2 이하 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-saint",
   "metadata": {},
   "source": [
    "# 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "documented-technical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 함. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   \n",
    "        # 우리 모델이 예측한 마지막 단어가 새롭게 생성한 단어가 됨.\n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여줌.\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "expired-judgment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m a <unk> <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence = \"<start> i love\", max_len = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-lover",
   "metadata": {},
   "source": [
    "# 루브릭 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-squad",
   "metadata": {},
   "source": [
    "1. 가사 텍스트 생성 모델이 정상적으로 동작하는가? 텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-layout",
   "metadata": {},
   "source": [
    "도출된 문장은 다음과 같습니다. \n",
    "\n",
    "< start > i love you , i m a < unk > < end > (임의로 띄어쓰기)\n",
    "\n",
    "결과는 많이 부족한 것 같습니다. 더 구체적인 가사가 나올 수 있을 것 같다는 생각이 듭니다. 모델은 정상작동 했으나, 표현이 풍부하지 못한 가장 큰 이유는 epoch를 5번 밖에 돌리지 못했기 때문으로 판단됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-reconstruction",
   "metadata": {},
   "source": [
    "2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가? 특수문자 제거, 토크나이저 생성, 패딩 처리 등의 과정이 빠짐없이 진행되었는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-union",
   "metadata": {},
   "source": [
    "전처리와 데이터셋 구성 과정은 체계적으로 진행되었다고 생각합니다. 제가 epoch를 5번 밖에 돌리지 못한 이유도 여기에 있습니다. 저는 계속 ```tf.data.Dataset``` 객체를 생성 과정이 ```train_test_split()```로 데이터셋을 분리하는 과정보다 앞이라고 생각했습니다. 그러니 dataset을 기껏 만들어놨것만, 훈련 데이터와 시험 데이터로 분리하는 데에 사용하지 않아서 dataset의 쓸모에 대해 생각하느라 제출 시간 임박까지 모델을 돌리지 못했습니다. 조원의 도움으로 ```tf.data.Dataset``` 객체를 생성 과정 데이터셋 분류 뒤에 일어나는 일임을 알게 되어 제출 시간이 임박해서야 모델을 돌려볼 수 있었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-milan",
   "metadata": {},
   "source": [
    "3. 텍스트 생성모델이 안정적으로 학습되었는가? 텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-catholic",
   "metadata": {},
   "source": [
    "학습 면에서는 평가 항목에 미달인 것 같습니다. 우선 Val-loss는 2.2 이하에 도달하지 못했습니다. 하이퍼파라미터를 조정하지 않았고, epoch도 5로 낮은 수기 때문입니다. 안정적으로 학습되었는지 평가하기는 어려운 것 같습니다. 그리고 우선 제출을 위해 모델을 돌리는 도중에 멈췄습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-pattern",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-commerce",
   "metadata": {},
   "source": [
    "일을 미뤄서 하면 안된다는 것을 배운 프로젝트였습니다. 나름 저번주 할 때는 굉장히 재밌었고, 또 응용해볼만한 좋은 노드라고 생각했습니다. 그리고 다른 것들보다 쉽다고 생각했습니다. 그게 큰 실수였던 것 같습니다. 제출시간이 다됐는데, epoch가 도는 시간을 미처 예상하지 못해서 모델을 채 돌리기도 전에 제출합니다! 모델 훈련 시간을 줄이는 방법... 앞으로는 연구해서 꼭 학습이 빨리 끝나는 모델을 만들겁니다.     \n",
    "이 노드는 나중에 자기소개서 써주는 인공지능으로 한번 만들어보고 싶습니다. 이 인공지능으로 자기소개서를 써서 내면 그 자체로 포트폴리오가 아닐까요...?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
